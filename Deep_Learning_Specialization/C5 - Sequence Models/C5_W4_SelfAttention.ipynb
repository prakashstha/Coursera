{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention - the Math\n",
    "\n",
    "\n",
    "Notes based on the blog on <a url = \"https://towardsdatascience.com/self-attention-5b95ea164f61\">Basics of Self-Attention</a> by Ioana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We want a machine learning system to learn the important relationships between words, much like how a human understands the meaning of words in a sentence. \n",
    "\n",
    "- In the Figure below, both you and I recognize that the word \"The\" refers to \"animal,\" and thus it should be strongly connected to that word.\n",
    "\n",
    "- The model should understand the relationships between \"animal,\" \"cross,\" \"street,\" and \"the\" because they are all related to \"animal\". This understanding is made possible through Self-Attention.⁴\n",
    "\n",
    "\n",
    "    <img src=\"images/9_transformer_self-attention_visualization.png\" alt=\"self_attention_viz\" width=\"500\" >\n",
    "\n",
    "- At its core, *Self-Attention* is a mechanism that transforms one sequence of vectors, **x**, into another sequence, **z** (see Figure below). \n",
    "\n",
    "- Each vector in the original sequence is essentially a numerical representation of a word. \n",
    "\n",
    "- The corresponding vector in **z** not only captures the representation of the word itself but also encodes its relationships with the surrounding words.\n",
    "\n",
    "    <img src=\"images/1_attention.webp\" alt=\"attention\" width=\"500\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can think of words as vectors within the entire space of words.\n",
    "- The direction of each word-vector carries some meaning.\n",
    "- Similarities and differences between vectors reflect similarities and differences between the words themselves.\n",
    "\n",
    "- Let's start by looking at the first three vectors and only looking in particular at how the vector for “cat” ($x2$) is transformed into $z2$. All of these steps will be repeated for each of the input vectors.\n",
    "    \n",
    "    1. Multiply the vector in focus, $x2$, with all other vectors in the sequence, including itself, i.e., perform a product of each vector and the transpose of $x2$ \n",
    "    2. This step is equivalent to a dot product, which measures the similarity between two vectors and is proportional to the cosine of the angle between them.\n",
    "\n",
    "\n",
    "    <img src=\"images/2_attention.webp\" alt=\"attention_2\" width=\"500\" >\n",
    "\n",
    "- The operation used to calculate the product between vectors is a hyperparameter we can choose. The dot product is the simplest option and the one used in \"Attention Is All You Need\".\n",
    "- We focus on one word at a time and determine its output based on its neighboring words. Currently, we look at the words before and after, but we could expand the window in the future.\n",
    "\n",
    "    $w'_{21} = \\text{X}_2^\\text{T}\\text{X}_1$\n",
    "\n",
    "    $w'_{22} = \\text{X}_2^\\text{T}\\text{X}_2$\n",
    "\n",
    "    $w'_{23} = \\text{X}_2^\\text{T}\\text{X}_3$\n",
    "- If the word in focus is \"cat,\" the sequence of words being considered is \"the,\" \"cat,\" and \"sat\". We’re determining how much attention the word \"cat\" should give to each of these words—“the,” “cat,” and “sat”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Multiplying the transpose of the word vector in focus with the sequence of surrounding words gives us a set of 3 raw weights. Each weight reflects how connected the two words are in meaning.\n",
    "- These raw weights need to be normalized for easier use.\n",
    "  - We normalize them using the **softmax formula** \n",
    "  - Softmax converts the numbers into a range of 0 to 1, where each output is proportional to the exponential of the input number.\n",
    "- This normalization makes the weights more useful and interpretable.\n",
    "\n",
    "    $w_{ij} = \\dfrac{e^{w'_{ij}}}{\\sum\\limits_{j} e^{w'_{ij}}}$\n",
    "\n",
    "    $w_{21} = \\dfrac{e^{w'_{21}}}{e^{w'_{21}} + e^{w'_{22}} + e^{w'_{23}}}$\n",
    "\n",
    "- Now, we take the normalized weights (one for each vector in the $j$ sequence).\n",
    "- Multiply each weight by its corresponding $x$ input vector.\n",
    "- Sum the products to get the output $z$ vector.\n",
    "- This process generates the output vector for $x2$ (\"cat\").\n",
    "- The same operation is repeated for every input vector in $x$ to obtain the complete output sequence.\n",
    "\n",
    "\n",
    "    <img src=\"images/4_attention_z.webp\" alt=\"attention_2\" width=\"500\" >\n",
    "\n",
    "\n",
    "- This explanation so far may raise some questions:\n",
    "\n",
    "    - Aren’t the weights we calculated highly dependent on how we determined the original input vectors?\n",
    "    - Why are we relying on the similarity of the vectors? What if we want to find a connection between two ‘dissimilar’ words, such as the object and subject of “The cat sat on the matt”?\n",
    "\n",
    "- In the next post, we’ll address these questions. We’ll transform each vector for each of its different uses and thus define relationships between words more precisely so that we can get an output more like Fig 2.8.\n",
    "\n",
    "    <img src=\"images/5_attention_words_relation.webp\" alt=\"attention_2\" width=\"500\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
