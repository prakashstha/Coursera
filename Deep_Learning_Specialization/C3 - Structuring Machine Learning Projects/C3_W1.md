
# ML Strategy

- This course focuses on **ML strategy**, aiming to teach efficient methods for improving ML systems.


## Why ML Strategy?

- When a system's performance is insufficient (e.g., a cat classifier at 90% accuracy), many improvement ideas arise, such as:
    - Collecting **more diverse training data**.
    - Training algorithms longer or using different **optimization algorithms** (e.g., Adam).
    - Experimenting with **network size** (bigger or smaller).
    - Applying **regularization techniques** (e.g., dropout, L2).
    - Changing **network architecture** (e.g., activation functions, hidden units).
- Without a sound strategy, time can be wasted pursuing ineffective ideas (e.g., six months collecting data for minimal improvement).

## Orthogonalization

**Orthogonalization** is a key strategy for efficiently tuning ML systems. It involves identifying and adjusting specific "knobs" or parameters that primarily affect **only one aspect of performance**, similar to how individual knobs on an old TV controlled only one picture setting (e.g., height or width).

* **The Problem:** ML systems have numerous changeable elements and hyperparameters. Without orthogonalization, a single adjustment might unintentionally affect multiple performance aspects, making tuning extremely difficult.
    * *Analogy:* Imagine a TV knob that simultaneously changes height, width, and rotation, or a car's joystick that mixes steering and speed control. Tuning these would be nearly impossible to achieve a desired state.

* **The Goal:** To have distinct sets of controls, or "knobs," for distinct issues. This makes it much easier to diagnose and fix problems.

* **Four Key Performance Areas (and their "Orthogonal" Knobs):**

    1.  **Fitting the Training Set Well (Low Bias):**
        * **Problem:** Algorithm doesn't perform well on the training data.
        * **Knobs:** Train a **bigger network**, use a **better optimization algorithm** (like Adam), etc.
        * *Analogy:* If the TV picture isn't wide enough, you use the "width" knob.

    2.  **Fitting the Development (Dev) Set Well (Low Variance):**
        * **Problem:** Algorithm performs well on the training set but poorly on the dev set (overfitting).
        * **Knobs:** Apply **regularization techniques** (e.g., L2, dropout), get **more training data**.
        * *Analogy:* If the TV picture's height is off *after* adjusting width, you use the "height" knob.

    3.  **Fitting the Test Set Well:**
        * **Problem:** Algorithm performs well on the dev set but poorly on the test set.
        * **Knobs:** Obtain a **larger dev set** (suggests you've over-tuned to your current dev set).

    4.  **Performing Well in the Real World:**
        * **Problem:** Algorithm performs well on the test set but fails in actual application (e.g., users aren't happy).
        * **Knobs:** Adjust the **dev/test set distribution** or **cost function** to better reflect real-world objectives.

* **Avoid Non-Orthogonal Controls:** Some techniques, like **early stopping**, are less orthogonal because they simultaneously affect multiple aspects (e.g., how well you fit the training set and dev set performance). While not inherently bad, they can complicate the tuning process.

* **Benefit:** By understanding what exactly is limiting your system's performance and having specific, orthogonal "knobs" to address that single bottleneck, you can much more effectively and quickly improve your ML system.

## Single Number Evaluation Metric

- It is better to a single evaluation metric for your project before you start.
* **Empirical Process Acceleration:** ML development is an iterative process: ideate, code, experiment, refine. 
    - A single metric speeds up this loop by providing clear feedback.
    - It allows you to quickly determine if new ideas, hyperparameter tunes, or algorithm changes are improving or worsening performance.

* **Problem with Multiple Metrics (e.g., Precision and Recall):**
    * Using a precision/recall for evaluation is good in a lot of cases, but separately they don't tell you which algothims is better. Ex:

    | Classifier | Precision | Recall |
    | ---------- | --------- | ------ |
    | A          | 95%       | 90%    |
    | B          | 98%       | 85%    |

    * A better thing is to combine precision and recall in one single (real) number evaluation metric. There a metric called F1 score, which combines them. You can think of F1 score as an average of precision and recall F1 = 2 / ((1/P) + (1/R))


* **Solution: Combine Metrics into One:**
    * Create a new metric that combines multiple performance indicators into a single score.
    * **F1 Score Example:** For precision (P) and recall (R), the **F1 score** (harmonic mean: $2 / (1/P + 1/R)$) is a standard way to average them into a single, decisive number. This allows for quick comparison and selection (e.g., Classifier A with a higher F1 score is clearly better than Classifier B).


* **Key Benefit:** A well-defined development (dev) set combined with a single real-number evaluation metric significantly **speeds up the iteration process** in ML projects, making decision-making more efficient.


## Optimizing and Satisficing Metrics

When a single evaluation metric can't capture all the important aspects of your ML system, using **optimizing** and **satisficing metrics** provides a practical solution for choosing the best model.

* **The Challenge:** It's hard to combine all desired qualities (e.g., accuracy and running time) into one formula without making arbitrary trade-offs.

* **The Solution: Optimizing and Satisficing Metrics**
    * **Optimizing Metric:** This is the primary metric you want to **maximize** or **minimize** as much as possible. You aim for the absolute best performance on this one.
    * **Satisficing Metrics:** These are secondary metrics that just need to meet a **certain threshold** or "be good enough." Once the threshold is met, further improvement beyond that point isn't a primary concern.

* **Example 1: Cat Classifier (Accuracy & Running Time)**
    * You might want to **maximize accuracy** (optimizing metric).
    * **Subject to** the running time being **less than or equal to 100 milliseconds** (satisficing metric).
    * This approach clearly guides model selection (e.g., pick the most accurate classifier that meets the 100ms speed requirement).

* **Example 2: Wake Word Detection (Accuracy & False Positives)**
    * You might want to **maximize wake word detection accuracy** (optimizing metric).
    * **Subject to** having **at most one false positive every 24 hours** (satisficing metric).
    * This ensures the device reliably wakes when called but doesn't randomly activate too often.

* **General Approach:** If you have N metrics, typically choose **one optimizing metric** and the remaining **N-1 as satisficing metrics** with defined thresholds.

* **Benefit:** This framework provides an **almost automatic way** to quickly evaluate multiple models and select the "best" one, even when dealing with diverse performance criteria. These metrics are always calculated on your training, development, or test sets.


## Train-Dev-Test Distribution

- Properly setting up your **development (dev) and test sets** is critical for efficient ML project progress. An ill-conceived setup can significantly slow down your team.

* **The ML Workflow:** You repeatedly train models on the **training set**, use the **dev set** to evaluate ideas and select the best model, iterate to improve dev set performance, and finally, evaluate the chosen model on the **test set**.

* **Crucial Rule: Dev and Test Sets Must Share the Same Distribution.**
    * **Problem with Different Distributions:** If your dev and test sets come from different data distributions (e.g., dev set from US/UK/Europe, test set from South America/India/China/Australia), your team might optimize for months to perform well on the dev set, only to find the model fails dramatically on the test set.
        * *Analogy:* It's like a team aiming at a bullseye for months, only for the target to be moved just before the final evaluation, invalidating all their focused effort.
    * **Real-World Example:** A loan approval team optimized their model on data from *medium-income zip codes* for months. When tested on *low-income zip codes*, their model performed poorly because the data distributions were vastly different, wasting significant time.

* **Correct Setup:**
    * **Shuffle All Data:** Randomly shuffle all available data (e.g., from all eight regions/countries, or all income levels) and then split it into dev and test sets.
    * **Reflect Future Data:** Both the dev and test sets should reflect the type of data you expect to encounter in the future and where you want your model to perform well.
    * **Consistent Target:** By having dev and test sets from the same distribution, you ensure your team is consistently aiming at the *actual target* you want your ML system to hit in the real world. This maximizes efficiency by ensuring improvements on the dev set directly translate to the desired real-world performance.

* **Key Takeaway:** The dev set, combined with your chosen evaluation metric, defines your target. Ensuring the dev and test sets come from the same distribution guarantees that the target you're aiming for during development is the same one you'll be measured against, saving months of wasted effort. (Training set setup will be discussed separately).

---
## Size of Dev and Test Set

The traditional 70/30 (train/test) or 60/20/20 (train/dev/test) data splits, common in earlier ML, are less relevant in the deep learning era due to **much larger datasets**.

* **Old Rule of Thumb (Small Datasets):**
    * For datasets with hundreds or thousands of examples, splits like 70/30 or 60/20/20 were reasonable.

* **New Rule of Thumb (Large Datasets - e.g., 1 Million Examples):**
    * Deep learning algorithms are **data-hungry**. It's now common and efficient to allocate a much larger fraction of data to training.
    * A split of **98% training, 1% dev, and 1% test** is perfectly reasonable.
    * Even 1% of a million examples (10,000 examples) is often **plenty large enough** for a robust dev or test set.

* **Purpose of the Test Set:**
    * The **test set** is for **final, unbiased evaluation** of your fully developed system.
    * It should be **large enough to give high confidence** in your system's overall performance. This might mean 10,000 examples, which is often far less than 30% of a massive dataset.
    * **Not having a separate test set** (i.e., only a train/dev split) is generally **not recommended**, as it can lead to unknowingly overfitting the dev set. While some teams might get away with it if their dev set is extremely large, a distinct test set provides crucial unbiased validation before deployment.

* **Dev Set Purpose:**
    * The **dev set** is used for **evaluating different ideas** and selecting the best model during the iterative development process. It should be large enough to effectively differentiate between competing models.

* **Key Trend:** In the era of big data, the trend is towards allocating **more data for training** and significantly **less for dev and test sets**, provided these smaller sets are still sufficient for their specific purposes (iterative evaluation and final unbiased assessment).


## When to Change Your Evaluation Metric and/or Dev/Test Set

Your **development (dev) set** and **evaluation metric** are like a target for your ML team. However, sometimes you discover this target is in the wrong place, and when that happens, you *must* move it.

* **Problem 1: Metric Doesn't Reflect True Performance/Preference**
    * **Scenario:** An algorithm (A) achieves lower error (e.g., 3%) than another (B, at 5%) on your current metric, but Algorithm A lets through unacceptable content (e.g., pornographic images in a cat app). Your users and company prefer Algorithm B because it avoids this critical flaw, despite its higher error rate.
    * **Issue:** Your evaluation metric is **misranking algorithms** â€“ it's identifying a technically "better" algorithm (A) that is actually worse for your application.
    * **Solution:** **Change your evaluation metric**.
        * You can introduce **weighting** to your error calculation. For instance, assign a much higher penalty (e.g., a weight of 10x or 100x) to misclassifications of critical content (like pornographic images). This ensures that errors on unacceptable content significantly increase the overall error score, making truly undesirable algorithms rank lower.
        * This is an example of **orthogonalization**: defining the metric is one distinct step ("placing the target"), and then optimizing the algorithm to perform well on that metric is a separate step ("aiming and shooting").

* **Problem 2: Dev/Test Set Distribution Doesn't Reflect Real-World Data**
    * **Scenario:** Your algorithm performs well on your dev/test sets (e.g., high-quality, well-framed images downloaded from the internet). However, when deployed in a real application, it performs poorly because users upload different types of images (e.g., blurrier, poorly framed, funny expressions).
    * **Issue:** Your evaluation data (dev/test sets) does **not accurately represent the data your algorithm will encounter in the real world**, making your development efforts misdirected.
    * **Solution:** **Change your dev/test set distribution**. Update your dev and test sets to include data that **reflects the real-world conditions and user inputs** your application will face.

* **General Guideline:** If your current **evaluation metric and/or dev/test set distribution** does not correspond to doing well on what you **actually care about** in your application, then **change them**.

* **Recommendation:** Even if you can't define the "perfect" metric or dev set from the start, **set something up quickly** to enable rapid iteration. It's perfectly fine to **modify it later** if you discover it's not effectively guiding progress. The key is to **avoid operating without any evaluation metric and dev set** for too long, as this significantly slows down development.


## Why Human-Level Performance?

* **Benchmark for ML Systems:** Comparing ML systems to human performance helps us evaluate their effectiveness.
* **Efficiency in Design:** Aiming to replicate human capabilities streamlines the design workflow for ML systems.

* **Rapid Progress to a Point:** ML progress is fast as algorithms approach human-level performance but slows after surpassing it.
* **Bayes Optimal Error:** This is the theoretical best performance achievable, the absolute lowest error rate, which no algorithm can surpass due to inherent data noise.


## Challenges After Surpassing Human Performance

* **Limited Improvement Room:** Human performance is often near the Bayes optimal error, leaving little space for further gains.
* **Loss of Improvement Strategies:** Once algorithms exceed human performance, methods like using human-labeled data for training become less viable.

---

## Avoidable Bias and Variance

* **Diagnostic Tool:** Human-level performance acts as a benchmark to determine whether to reduce **bias** or **variance**.
* **Bias Reduction:**
    * **When:** Training error is significantly higher than human-level performance.
    * **Strategies:** Train larger neural networks, extend training duration, use advanced optimizers (Adam, RMSprop), experiment with architectures/hyperparameters.
* **Variance Reduction:**
    * **When:** Training error is close to human-level performance, but development/test error is higher.
    * **Strategies:** Increase training data, implement regularization (L2, dropout, data augmentation).

---

## Understanding Human-Level Performance

* **Contextual Definition:** Human-level error is often a proxy for Bayes error, but its definition can vary.
* **Example (Medical Imaging):**
    * Untrained humans: 3% error
    * Typical doctors: 1% error
    * Experienced doctors: 0.7% error
    * Team of experienced doctors: 0.5% error (often a proxy for Bayes error)
* **Impact:** The chosen definition influences bias/variance analysis in ML projects.

---

## Surpassing Human-Level Performance

* **Increased Difficulty:** As ML algorithms approach human-level, identifying further improvements becomes harder.
* **Examples of Success:** ML has surpassed human performance in areas with structured data, like online advertising, product recommendations, logistics, and loan approvals.

---

## Improving Model Performance

* **Systematic Approach:** Focus on addressing **avoidable bias** (fitting the training set well) and **variance** (generalizing to new data).
* **Bias Strategies:** Train larger models, use advanced optimization algorithms, experiment with architectures.
* **Variance Strategies:** Increase training data, apply regularization techniques (L2, dropout, data augmentation).