## Error Analysis

### Carrying Out Error Analysis
When your machine learning model isn't performing as well as humans on a task, **error analysis** helps you figure out what to fix next. It's about looking at your model's mistakes to understand them.

- **How it Works:**
    1.  **Gather Mistakes:** Take about 100 examples your model got wrong from your development set.
    2.  **Look Closely:** Manually review each of these wrong examples.
    3.  **Categorize Errors:** Group the mistakes by type (e.g., misidentifying dogs as cats, blurry images, images of large wild cats).
    4.  **Count Them Up:** See what percentage of your errors fall into each category.

- **Why it Matters (Example: Cat Classifier with 10% Error):**
    - If only 5% of errors are from misclassifying dogs, even if you perfectly solve that, your overall error only drops from 10% to 9.5%. It might not be worth a huge effort.
    - But if 50% of errors are from dogs, fixing that could cut your error from 10% to 5%. This is a big win and likely worth the effort.

- **Key Benefits:**
    - **Quick Decisions:** A short manual review (5-10 minutes for 100 examples) gives you a fast estimate of what problems are most impactful to solve.
    - **Focus Your Efforts:** It helps you prioritize where to spend your development time, so you're not wasting effort on minor issues.
    - **Discover New Problems:** You might even find new types of errors you hadn't considered before (like problems with "Instagram filters").

* **Putting it into Practice:**
    - Use a simple spreadsheet to list misclassified images and mark which error categories apply

    - Sometimes, you can evaluate multiple error analysis ideas in parallel and choose the best idea. Create a spreadsheet to do that and decide, e.g.:

    | Image        | Dog    | Great Cats | blurry  | Instagram filters |    Comments    |
    | ------------ | ------ | ---------- | ------- | ----------------- |--------------- |
    | 1            | ✓      |            |         | ✓                 |  Pitbull       |
    | 2            | ✓      |            | ✓       | ✓                 |                |
    | 3            |        |            |         |                   |Rainy day at zoo|
    | 4            |        | ✓          |         |                   |                |
    | ....         |        |            |         |                   |                |
    | **% totals** | **8%** | **43%**    | **61%** |      **12%**      |                |

    - In the last example you will decide to work on great cats or blurry images to improve your performance.
    - This quick counting procedure, which you can often do in, at most, small numbers of hours can really help you make much better prioritization decisions, and understand how promising different approaches are to work on. 

### Handling Incorrectly Labeled Data

- It's common to find **incorrectly labeled examples** in your datasets (where the human-assigned `Y` label is wrong for a given `X` input). How you handle these depends on which dataset they're in: train, dev or test.
* **DL algorithms are generally robust to *random* errors** in the training set. If labels are wrong due to simple mistakes (e.g., accidental key presses) and not a systematic bias, it's often okay to leave them as is, especially if you have a large dataset and the error percentage isn't too high.
* **However, be wary of *systematic* errors.** If a labeler consistently mislabels a specific type of image (e.g., all white dogs as cats), this will negatively impact your model's learning.
    
* **Training Set:**
    * While fixing training set labels isn't harmful, the effort might not always be worth the small gain, given the robustness of DL algorithm to random noise.

* **Development (Dev) and Test Sets:**
    * **Incorrect labels here are more critical** because these sets are used to evaluate and compare models. If they're inaccurate, you might pick the wrong model.
    * **During error analysis, add a column** to track "incorrectly labeled examples." This helps you quantify how much of your model's apparent error is actually due to bad labels.
    * **Deciding to Fix:**
        * **Fix if it significantly impacts evaluation:** If incorrect labels are a large fraction of your overall error (e.g., 30% of mistakes are due to bad labels), or if they prevent you from confidently choosing between two similar-performing models (e.g., 2.1% vs 1.9% error), then fix them.
        * **Don't fix if impact is small:** If incorrect labels account for a tiny percentage of overall error (e.g., 0.6% of a 10% total error), fixing them might not be the most efficient use of your time right now.

* **Guidelines for Fixing Dev/Test Set Labels:**
    1.  **Apply Consistently to Dev and Test Sets:** If you fix labels in your dev set, apply the exact same process to your test set. This ensures both sets remain from the **same distribution**, which is crucial for reliable evaluation.
    2.  **Consider Correctly Labeled Examples Too (Optional but Ideal):** Ideally, you'd check both correctly and incorrectly predicted examples for labeling errors. This avoids introducing bias by only correcting mistakes your model made. However, this can be time-consuming if your model is highly accurate (98% correct means checking 98% of data), so it's not always done.
    3.  **Training Set vs. Dev/Test Set Corrections:** It's okay to only correct labels in your (often smaller) dev and test sets without extending the same effort to your (often much larger) training set. DL models are fairly robust to differences in training vs. dev/test set distributions, provided the dev/test sets themselves are consistent.

* **Final Advice on Manual Inspection:**
    * While some researchers may prefer "pure" algorithmic approaches, **manual error analysis and human insight are invaluable for building practical ML systems.**
    * Don't be reluctant to manually examine examples. Even a few minutes or hours spent reviewing data can significantly help you **prioritize what to work on next**, making it a highly effective use of your time.


### Build Your First ML System Quickly, Then Iterate

- When starting a **brand new ML project**, the best approach is to **build your first system quickly** and then improve it step-by-step.

* **The Challenge:** For most ML applications, there are many potential directions to take (e.g., in speech recognition: handling noise, accents, far-field speakers, or children's speech). Deciding where to focus can be overwhelming.

* **Recommended Strategy:**
    1.  **Set a Target Fast:** Quickly define your **development (dev) and test sets** and a **single evaluation metric**. This sets a clear goal, even if it's not perfect initially (you can adjust it later).
    2.  **Build a Quick, Initial System:** Get a basic ML model up and running as soon as possible. Don't overthink it or try to make it perfect. Find some training data and train the model.
    3.  **Analyze and Prioritize:** Once you have a working system, even a "quick and dirty" one, you can:
        * Perform **bias/variance analysis** to understand if your model is underfitting or overfitting.
        * Conduct **error analysis** (manually examining mistakes) to identify the biggest problems and their root causes (e.g., if many speech recognition errors are from far-field audio, you know to focus on that).
    4.  **Iterate:** Use the insights from your analysis to decide which specific problems to tackle next, and then repeat the process: build, analyze, prioritize, improve.

* **Common Mistake:** Many teams **overthink** the initial system, making it too complicated from the start. It's usually better to start simple and let analysis guide complexity.

* **Overall Goal:** If your main goal is to build a **working system** (not to invent a new ML algorithm), then building a quick initial version, performing bias/variance and error analysis, and using those results to prioritize is the most effective path to success.


## Mismatched Train/Dev/Test Set Distribution

### Training and Testing on Different Distributions
- DL algorithms are "data-hungry," leading many teams to use all available training data, even if some of it comes from a different source than their dev and test sets. This is a common scenario in modern DL.

- **The Challenge**: You need your DL system to perform well on a specific data distribution (e.g., photos from a mobile app), but you might have limited data from this source and much more from a different, broader source (e.g., high-quality web images).

- **Why Not Just Mix All Data?**: Simply mixing all your data and then randomly splitting it into training, dev, and test sets is a bad idea. Your dev and test sets would then mostly reflect the broader, less relevant data source, leading your team to optimize for the wrong goal.

- **The Recommended Approach**

    - **Training Set**: Use a large mix of data. Include all your general data (e.g., 200,000 web images) and a portion of your specific target data (e.g., 5,000 mobile app images).
    - **Dev and Test Sets**: Crucially, these should only contain data from your target distribution (e.g., 2,500 mobile app images for dev, 2,500 for test).
    - This strategy ensures your dev and test sets accurately reflect the real-world performance you need, even if your training data's distribution is a bit different. While the training data might not perfectly match your dev/test sets, this setup generally leads to better overall model performance.

- **In Essence**: It's okay for your training data to have a different distribution than your dev and test sets, especially if it means a much larger training set. The key is that your dev and test sets must accurately reflect the specific data your model will face in its final application. This ensures your team is always optimizing for the right outcome.

### Bias and Variance with Mismatched Data Distribution

- Bias and Variance analysis changes when training and Dev/test set is from the different distribution.
- Lest take the cat classification example and assume you have reached to the following result
    
    * Human Error: 0% (perfect)
    * Training Error: 1% (model does well on training photos)
    * Dev Error: 10% (model does much worse on development photos)
    

- Normally, a big jump from training to dev error would mean a variance problem (your model isn't generalizing). But if your training photos were super clear and your dev photos were blurry phone pics, the problem might not be just variance; the data itself is different.

- **The Solution: Add a "Train-Dev" Set**: To figure out the real issue, we create a train-dev set. This is a random small part of your training data that your model doesn't actually train on. It helps us see how well the model generalizes to new but similar data.

- Let's look at numbers with a train-dev set:

- **Scenario 1: High Variance**
    - Human Error: 0%
    - Training Error: 1%
    - Train-Dev Error: 9% (Big jump from Training)
    - Dev Error: 10% (Small jump from Train-Dev)
    - Conclusion: This shows a clear variance problem. Your model is doing great on the data it saw during training but struggles with new, similar data.
- **Scenario 2: Data Mismatch**
    - Human Error: 0%
    - Training Error: 1%
    - Train-Dev Error: 1.5% (Small jump from Training)
    - Dev Error: 10% (Huge jump from Train-Dev)
    - Conclusion: This points to a data mismatch problem. Your model generalizes well to new, similar data (train-dev) but struggles when the data type changes (dev set).

- By comparing error rates across these sets, you can identify specific issues:

    1. Human-level error (proxy for Bayes error)
  2. Train error
     - `avoidable bias = training error - human level error`
     - If the difference is big then its **Avoidable bias** problem.
  3. Train-dev error
     - `variance = training-dev error - training error`
     - If the difference is big then its high **variance** problem.
  4. Dev error
     - `data mismatch = dev error - train-dev error`
     - If difference is much bigger then train-dev error its **Data mismatch** problem.
  5. Test error
     - `degree of overfitting to dev set = test error - dev error`
     - Is the difference is big (positive) then maybe you need to find a bigger dev set (dev set and test set come from the same distribution, so the only way for there to be a huge gap here, for it to do much better on the dev set than the test set, is if you somehow managed to overfit the dev set).

### Addressing Data Mismatch
- If your training data is different from your dev/test data and you have a data mismatch problem, here's what you can try:

    1. **Understand the Differences**
        - *Manual Error Analysis*: Look (or listen) closely at examples your model gets wrong in the dev set.
        - *Identify Patterns*: Figure out how your dev set data is different or harder than your training data. For example, are dev set speech recordings much noisier? Do they contain more specific terms (like street numbers)?
    
    2. **Make Training Data More Similar**: 
        - Once you know the differences, try to make your training data more like your dev/test data.

        - *Collect More Similar Data*: If possible, gather more training examples that match your dev/test set's characteristics (e.g., more recordings of people saying numbers).

        - *Artificial Data Synthesis*: This is a powerful technique where you create synthetic data that mimics the target distribution. *Example (Speech)*: If car noise is a problem, take clean speech recordings and digitally add car noise to them. This makes your training data sound more like real in-car audio.


- **Caution with Artificial Data Synthesis**
    - *Overfitting*: Too little variety (e.g., repeating the same car noise) can make your DL model learn only those specific examples, not general concepts.
    - *Limited Realism*: Even if synthesized data looks real, it might cover a tiny fraction of real-world variations, leading your model to overfit to that narrow subset.

- **In Short**, when facing a data mismatch, the main strategy is to manually analyze the differences between your training and dev/test data. Then, either collect more data that matches your target, or use artificial data synthesis to make your training data more similar. Just be mindful of the potential for limited variety when creating synthetic data.


## Learning for Multiple Task

### Transfer Learning

- Transfer learning is a powerful DL technique where a neural network learns knowledge from one task and applies it to a different, often related, task.

- **How it Works?**
    - **Pre-training**: Train a neural network on a task with lots of data (e.g., image recognition with millions of images). This teaches the network to recognize basic features like edges, curves, or sounds.
    - **Transfer**: Take this pre-trained network. Remove its last output layer.
    - **Fine-tuning**:
        - Add a new output layer (or a few new layers) for your new task (e.g., radiology diagnosis using X-ray scans, or wake word detection for voice assistants).
        - Initialize these new layers with random weights.
        - Train the network on your new task's data.
        - If you have little data for the new task, you might only retrain the new last layer(s), keeping the pre-trained layers "frozen."
        - If you have more data, you can "fine-tune" by retraining all layers of the network.

- **When Transfer Learning Makes Sense?**

    - **Same Input Type**: Both tasks use the same type of input (e.g., both use images, or both use audio).
    - **Lots of Data for Task A, Less for Task B**: You have a large dataset for the initial "transfer from" task (Task A) but relatively less data for the "transfer to" task (Task B). The rich knowledge gained from Task A helps compensate for limited data in Task B.
    - **Useful Low-Level Features**: You suspect that the basic features learned in Task A (e.g., recognizing image components, understanding human speech) will be helpful for solving Task B.

- **When It Might Not Help Much?**
    - You have more or comparable data for Task B than for Task A. The "transferred" knowledge might not add significant value over just training directly on your Task B data.

### Multi-Task Learning
- Multi-task learning is when a single neural network learns to perform several related tasks simultaneously, with each task ideally helping the others. This differs from transfer learning, which is sequential.

- **How it Works (Example: Self-Driving Cars)**
    - Instead of training separate DL models to detect pedestrians, cars, stop signs, and traffic lights, you train one neural network to predict all these things from a single image input (image has multiple labels).
  - Y will have a shape of `(4,m)` because we have 4 classes and each one is a binary one.
  - Then 

    $\mathbf{Cost} = \dfrac{1}{m} * \large\sum\limits_{i = 1}^{m}\sum\limits_{j = 1}^{4}L(\hat{y}_j^{(i)}, y_j^{(i)})$

     where   
    $L = - y_j^{(i)} * log(\hat{y}_j^{(i)}) - (1 - y_j^{(i)}) * log(1 - \hat{y}_j^{(i)})$

* Instead of training a separate DL model for each task, you can train a single neural network to do all four. This works better because the network can learn common features that help with all the detection jobs at once.
* Multi-task learning also works even if some labels are missing for certain tasks in your data. The model simply learns from the labels it has. For example:
  ```
  Y = [1 ? 1 ...]
      [0 0 1 ...]
      [? 1 ? ...]
  ```
  - In this case, the loss function will be:   
    $Loss = \dfrac{1}{m} * \large\sum\limits_{i = 1}^{m}\sum\limits_{j = 1}^{4} L(\hat{y}_j^{(i)}, y_j^{(i)}) \; \;\text{where}\; \; y_j^{(i)} != ?$
    

### When Multi-Task Learning Makes Sense

* **Shared Low-Level Features**: The tasks can benefit from the network learning common basic features (e.g., detecting different objects on a road all rely on understanding visual patterns).

* **Similar Data Amount Per Task (Often)**: While not a strict rule, it often works well when you have a similar amount of data for each task. The collective data from all tasks provides a much larger training set for any single task, giving it a significant boost. For instance, if you have 100 tasks with 1,000 examples each, a single task benefits from the combined 99,000 examples from the other 99 tasks.

* **Large Enough Network**: You can train a neural network that's big enough to handle all the tasks effectively. If the network is too small, it might hurt performance compared to training separate networks.

### Multi-Task Learning vs. Transfer Learning
* Transfer learning (pre-training then fine-tuning) is generally used much more often in practice, especially when you have a problem with limited data that can benefit from knowledge gained from a related, data-rich task.

- Multi-task learning is less common, as it's harder to find many tasks that truly benefit from simultaneous training in a single network. Object detection in computer vision is a notable exception where it's frequently applied.
- Both are valuable tools, but transfer learning tends to be more widely applicable for common ML challenges.

## End-to-End DL: One Network, One Goal

### What is End-to-End DL?
- **End-to-end DL** means replacing multi-stage processing systems with a single neural network that takes raw input (X) and directly produces the final output (Y).

- **How it Works (Speech Recognition Example)** Traditionally, speech recognition involved many steps: extracting features, finding phonemes (basic sounds), combining phonemes into words, and finally creating a transcript. End-to-end DL bypasses all these steps: you feed the raw audio directly into a large neural network, and it outputs the transcript.

    `Audio ---> Features --> Phonemes --> Words --> Transcript    # non-end-to-end system`

    `Audio ---------------------------------------> Transcript    # end-to-end deep learning system`

- **When End-to-End DL Excels**
    - **Lots of Data**: End-to-end DL usually requires a very large dataset to work well. For example, in speech recognition, it shines with tens or hundreds of thousands of hours of audio. With smaller datasets, traditional multi-stage approaches often perform better.
    - **Simplifies Systems**: It can significantly simplify system design by removing many hand-designed components and intermediate steps.

- **When Multi-Stage Approaches are Better**

    - **Limited Data**: If you don't have enough data for the end-to-end task, breaking it down into simpler sub-problems (each with more available data) can yield better results.
    
    - **Example: Face Recognition**: Instead of directly identifying a person from a raw image, the best current systems first detect and crop the face, then identify the person from the cropped face. This works better because there's ample data for both "face detection" and "face identity" as separate problems, but not enough data for the full end-to-end task.

        `Image ---------------------> Face recognition    # end-to-end deep learning system`

        `Image --> Face detection --> Face recognition    # deep learning system - best approach for now`

    - **Example: X-ray Age Estimation**: Estimating a child's age from an X-ray works better by first identifying and measuring bones, then using those measurements to estimate age, because there isn't enough data for a direct image-to-age end-to-end approach.

        `Image --> Bones --> Age    # non-end-to-end system - best approach for now`
        `Image ------------> Age    # end-to-end system`


### Pros and Cons of End-to-End DL
- End-to-end DL simplifies complex systems by using one large neural network to map directly from raw input (X) to final output (Y).

- **Benefits**

    - **Data-Driven Learning**: It lets the data "speak". A large enough neural network can figure out the best way to transform X to Y without human assumptions (e.g., it doesn't need to be forced to use "phonemes" for speech).

    - **Less Hand-Designing**: You spend less time manually designing features or intermediate steps, simplifying your workflow.

- **Disadvantages**
    - **Requires Lots of Data**: End-to-end DL needs a huge amount of X-Y data to learn complex mappings effectively. Often, there's more data available for individual sub-tasks than for the full end-to-end problem (e.g., face detection vs. identifying a person from a raw scene).

    - **Excludes Useful Manual Design**: While "hand-designing" might sound old-fashioned, it's how humans inject valuable knowledge when data is scarce. End-to-end DL largely skips this, which can be a drawback if you don't have enough data for the network to learn everything itself.

- **Conclusion**: End-to-end DL can be powerful and simplify systems, but it's not a cure-all. Its success heavily depends on having sufficient data to learn the entire complex mapping. Often, a combination of DL for well-defined sub-problems and traditional methods for other parts yields better results.